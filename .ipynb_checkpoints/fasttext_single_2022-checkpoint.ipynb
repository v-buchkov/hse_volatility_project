{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8495324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0da7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6ee630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm>=4.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.63.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tqdm>=4.9.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386b2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcbc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/fastText.git\n",
    "# !cd fastText\n",
    "# !pip install fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d28100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ccf1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0f7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download_model in module fasttext.util.util:\n",
      "\n",
      "download_model(lang_id, if_exists='strict', dimension=None)\n",
      "    Download pre-trained common-crawl vectors from fastText's website\n",
      "    https://fasttext.cc/docs/en/crawl-vectors.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.util.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cdbfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# fasttext.util.download_model('ru', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b2382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06434693, -0.01527086, -0.06963537, -0.03582602,  0.01471584,\n",
       "       -0.03503159,  0.02701715,  0.04161827, -0.00033126,  0.00355259,\n",
       "        0.06979205,  0.06205348,  0.05154078,  0.03831509, -0.02394784,\n",
       "       -0.03954181, -0.00189653, -0.11174394, -0.0407712 ,  0.09289949,\n",
       "       -0.07412342, -0.05209147,  0.02017231,  0.04837443,  0.02212641,\n",
       "        0.00856511, -0.03055364,  0.04733564,  0.04380886,  0.03856769,\n",
       "        0.03442968,  0.05576854,  0.01513439,  0.14055566,  0.03365337,\n",
       "       -0.02920472, -0.10305687, -0.09332671,  0.03085899, -0.11067575,\n",
       "       -0.08992791,  0.05850704, -0.017424  ,  0.00120653, -0.07153153,\n",
       "        0.10312843, -0.08066262, -0.00642456,  0.04408539, -0.05728461,\n",
       "       -0.0179531 ,  0.03936698,  0.04778077, -0.04907751, -0.00909553,\n",
       "        0.05588715, -0.00236535,  0.04878682, -0.01769035,  0.03295048,\n",
       "        0.00906604,  0.08772802,  0.02970458, -0.04903899, -0.03025401,\n",
       "       -0.04151824,  0.04931813, -0.02804473,  0.05716789,  0.03559401,\n",
       "       -0.12191223,  0.02087349, -0.05121018, -0.0584691 , -0.04781278,\n",
       "       -0.06298476, -0.00432743, -0.03785646, -0.08833752, -0.0375172 ,\n",
       "        0.04602968,  0.02096615,  0.00321184, -0.00927999, -0.00288017,\n",
       "        0.04345381, -0.0330169 ,  0.00840916, -0.05537616, -0.02134524,\n",
       "       -0.03705332,  0.06453154, -0.01733523, -0.01977487, -0.02836509,\n",
       "        0.01901042,  0.04043126, -0.07048826, -0.09381784, -0.02532577,\n",
       "       -0.02679786,  0.01097633, -0.01681483, -0.08134623,  0.00429079,\n",
       "       -0.07213577, -0.03950587,  0.07274695, -0.00337509,  0.05469057,\n",
       "       -0.01510826, -0.05297935,  0.04232059, -0.04494021, -0.01873806,\n",
       "        0.02970697, -0.02128338, -0.07461107,  0.04457341,  0.02913763,\n",
       "       -0.05406609,  0.06825955, -0.0423348 , -0.01933457,  0.00638132,\n",
       "        0.00075826,  0.10154837, -0.06699109, -0.01374834,  0.10683898,\n",
       "        0.06719182,  0.00299954,  0.03092229, -0.01919586,  0.02315286,\n",
       "        0.02552165,  0.0297376 ,  0.0476847 , -0.06794806,  0.01934321,\n",
       "        0.07793375,  0.04631811,  0.07487484, -0.06923444, -0.09797966,\n",
       "       -0.02230856,  0.04383751,  0.05814477,  0.09182699,  0.0407513 ,\n",
       "        0.06562199,  0.06420117, -0.12618978, -0.00895569, -0.03637737,\n",
       "        0.0323772 ,  0.05442533,  0.02233687,  0.0607053 , -0.03511162,\n",
       "       -0.02011008, -0.04657565, -0.1363746 , -0.09366813, -0.01257268,\n",
       "       -0.0822741 ,  0.04026463,  0.08941573,  0.05416025, -0.00148568,\n",
       "        0.02470817, -0.01521165,  0.06688396,  0.01970377, -0.067048  ,\n",
       "        0.05173868, -0.06437217,  0.02638604,  0.02355881, -0.03286408,\n",
       "       -0.01542088,  0.0226214 ,  0.01009578, -0.06503511,  0.05164307,\n",
       "        0.08621447, -0.00291589,  0.0201317 ,  0.05789564,  0.04330945,\n",
       "       -0.01468945,  0.00915974,  0.02692279,  0.07124459, -0.05370982,\n",
       "        0.04218086,  0.00314814,  0.00356758, -0.02068229,  0.0604349 ,\n",
       "       -0.08158811,  0.04939371,  0.0430281 , -0.03372736, -0.0558867 ,\n",
       "        0.00376545, -0.037087  ,  0.05940549,  0.02495521, -0.00334628,\n",
       "        0.01005986, -0.02053031,  0.01179219,  0.07010209, -0.10397089,\n",
       "        0.07733957,  0.06056703,  0.01003617, -0.15787463, -0.01765688,\n",
       "        0.00765593, -0.01905038,  0.01327723,  0.01084452, -0.05930451,\n",
       "       -0.07062402, -0.08540855,  0.01374613, -0.03077546, -0.04025275,\n",
       "        0.00268231, -0.06844234,  0.05945092,  0.02234607,  0.14669767,\n",
       "        0.03161074, -0.03626112,  0.13065669, -0.02795461,  0.00260858,\n",
       "        0.02103085, -0.01555263,  0.02790887, -0.02395738, -0.10259839,\n",
       "       -0.01162906,  0.07939966,  0.01229805,  0.0472665 ,  0.00792722,\n",
       "        0.07495239, -0.04777352,  0.02290227, -0.02349729, -0.01035224,\n",
       "       -0.04128299, -0.05037197, -0.02936148, -0.03995599, -0.09582971,\n",
       "        0.00652219, -0.00309965,  0.09921778, -0.07993053, -0.07525942,\n",
       "       -0.03148453, -0.03216294,  0.00986726, -0.03059178, -0.01402058,\n",
       "        0.02519003, -0.04734642,  0.12286284, -0.09953459,  0.01202545,\n",
       "        0.03934894,  0.07455902,  0.02744922,  0.03999532, -0.05147249,\n",
       "       -0.0055727 ,  0.07064351,  0.07526451,  0.00223117,  0.01765039,\n",
       "       -0.02785274,  0.0251571 ,  0.05081079,  0.06183068, -0.03087618,\n",
       "       -0.00268458, -0.02822061, -0.05344585, -0.05139395,  0.00151552,\n",
       "       -0.01931686,  0.00034288, -0.01423903,  0.00377267, -0.0598783 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft['–ø—Ä–∏–≤–µ—Ç']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fecc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def get_tweet_embedding(lemmas, model, embedding_size=300):\n",
    "    \n",
    "    res = np.zeros(embedding_size)\n",
    "    cnt = 0\n",
    "    for word in lemmas.split():\n",
    "        if word in model:\n",
    "            res += np.array(model[word])\n",
    "            cnt += 1\n",
    "    if cnt:\n",
    "        res = res / cnt\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ef304c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.84749218e-02,  1.14055865e-02, -1.54750008e-02,  6.10717852e-03,\n",
       "       -5.42343501e-03,  2.83443742e-03,  2.40256451e-03,  1.29073053e-02,\n",
       "        3.05031866e-02, -1.99234379e-02,  6.13203850e-02,  4.42768331e-02,\n",
       "        2.71531800e-02, -1.02064133e-02,  9.22483567e-04,  2.50384058e-02,\n",
       "       -1.25383004e-02, -4.89095808e-02, -3.07890818e-02,  1.01918663e-01,\n",
       "       -2.85800546e-02, -1.05811988e-01, -1.28629373e-02,  2.95597422e-02,\n",
       "        2.13206490e-03,  1.26906892e-02, -2.97227059e-02,  2.77029723e-02,\n",
       "       -1.21254625e-02, -4.76178443e-02, -6.68591424e-03,  3.05985650e-02,\n",
       "        3.59081652e-02,  1.02970391e-01,  3.62780495e-02, -5.56655712e-02,\n",
       "       -1.11200343e-01, -1.16946280e-01,  4.69890856e-02, -5.79430675e-02,\n",
       "       -4.56299540e-03, -2.32621958e-03, -2.30524363e-03,  1.96370891e-02,\n",
       "       -1.68996924e-02,  4.77626729e-02, -7.71877861e-02,  2.95996453e-02,\n",
       "        3.40769021e-02, -3.43663241e-02,  5.55797149e-02,  1.05126291e-02,\n",
       "        9.77615127e-03,  3.99762448e-02, -2.15159254e-02,  1.93620831e-02,\n",
       "       -1.34500194e-02,  2.17238814e-03,  1.39238648e-02, -1.24853794e-02,\n",
       "       -5.08000248e-02,  6.46954067e-02,  3.07824350e-02, -4.07074159e-02,\n",
       "       -1.99913508e-02,  1.96010725e-02, -1.52623175e-02, -1.58499312e-02,\n",
       "       -4.72955415e-02,  4.77682170e-03, -5.21384678e-02,  3.73177927e-02,\n",
       "       -3.41336881e-02,  5.90577209e-03,  6.41630217e-03, -2.70765051e-02,\n",
       "       -5.57585534e-02, -9.87857254e-03, -1.75393135e-02, -3.92743144e-02,\n",
       "        2.59191706e-02,  3.97690022e-02,  5.75262937e-03, -4.60929359e-02,\n",
       "        3.89499759e-03,  3.79725420e-02, -6.47511277e-02,  5.06016524e-02,\n",
       "        1.29573480e-02,  9.16208606e-04, -1.10791333e-01, -1.59734851e-02,\n",
       "        4.38956684e-03, -5.00591882e-02, -1.91273557e-02, -1.31234950e-02,\n",
       "        4.23284452e-02, -1.32317897e-02, -9.03501306e-02,  4.14077961e-02,\n",
       "       -9.01901850e-03,  7.61899361e-02, -1.04387742e-02, -1.22785277e-02,\n",
       "       -1.63511078e-02, -3.89259742e-02,  4.25888405e-02,  2.55549918e-02,\n",
       "        7.98778998e-03,  9.49782273e-03, -3.10476529e-02, -1.09063331e-02,\n",
       "        8.43151626e-02, -3.58087434e-02, -1.24211812e-02, -1.87083688e-02,\n",
       "       -5.45124998e-02, -8.86561619e-02,  3.79863534e-02, -3.31068560e-02,\n",
       "       -4.89519509e-02,  7.88756466e-02, -5.50974393e-02,  1.60968699e-02,\n",
       "        1.01266545e-03,  2.75706507e-02,  5.04551297e-02, -1.64009025e-02,\n",
       "        1.19224258e-02,  6.78465860e-02,  2.69761501e-02,  1.12457192e-02,\n",
       "        1.86175751e-02, -1.40191410e-02, -1.72757215e-02,  5.70655339e-02,\n",
       "        5.77574829e-04,  5.24380505e-02, -5.08161918e-03,  3.53807122e-02,\n",
       "       -1.40902330e-02,  2.29508245e-02,  6.68836981e-02, -2.08306434e-02,\n",
       "       -5.37150722e-02, -9.75077925e-03, -4.46044868e-02,  3.50373158e-02,\n",
       "        5.37473205e-02,  2.23793560e-02,  3.78368636e-02,  1.91988172e-02,\n",
       "       -9.24861385e-02,  1.07819675e-02, -1.21989548e-02,  3.31418150e-02,\n",
       "        2.23143799e-02,  1.49282608e-02,  1.40205264e-02, -2.72367133e-02,\n",
       "       -2.69449629e-02,  2.49509839e-03, -5.19380664e-02, -6.78124642e-02,\n",
       "       -6.38052975e-02,  1.68222875e-02, -7.74696337e-02,  8.44685482e-02,\n",
       "        4.74610087e-02, -2.62326931e-03, -9.03137206e-03, -6.54254213e-02,\n",
       "        4.35586069e-02,  3.72354283e-02, -3.41950073e-02, -1.16772181e-02,\n",
       "       -3.14856721e-02,  6.10582798e-02,  4.79020393e-02, -4.82062241e-02,\n",
       "       -1.99088580e-02,  3.10592290e-02, -4.53291640e-02, -6.53951182e-02,\n",
       "        6.01363019e-04,  3.27027021e-02,  1.47671076e-02, -3.70643544e-02,\n",
       "       -2.04997172e-02,  5.32348915e-02, -4.50411410e-02,  1.28656339e-02,\n",
       "        3.19320844e-02,  1.45057300e-02, -1.72404865e-02,  3.71150244e-02,\n",
       "       -2.15479551e-02, -1.15545052e-02, -1.25362631e-02,  2.60218652e-02,\n",
       "       -4.58280314e-02,  7.53374584e-03,  7.35389721e-03, -1.82377142e-02,\n",
       "       -9.48007656e-02, -2.56354164e-02,  5.40375593e-04,  4.41152531e-02,\n",
       "       -7.10717402e-04,  4.32135077e-03,  2.78144046e-02, -4.11721691e-03,\n",
       "       -8.98516446e-04,  5.58417288e-02, -3.63211357e-02, -6.08586776e-03,\n",
       "        3.32013650e-02,  3.86003682e-02, -4.08708490e-02,  1.05813891e-02,\n",
       "        1.79253643e-02,  3.97888436e-02,  3.22979216e-02,  1.34046650e-02,\n",
       "       -3.69763831e-02,  3.80074000e-02, -3.84184653e-02, -1.46874161e-02,\n",
       "        1.31309093e-02, -1.08475601e-02,  1.34081137e-02, -1.17426082e-02,\n",
       "        1.28444180e-01, -2.20539912e-02,  1.15903737e-01,  5.90186799e-03,\n",
       "        1.24620628e-02,  7.31505433e-02, -7.99386785e-03,  3.64330948e-02,\n",
       "        3.45786135e-02,  3.46622248e-02,  2.36364873e-03, -7.05305371e-02,\n",
       "       -5.42092409e-02, -8.53608083e-03, -5.25069842e-03, -6.02257324e-02,\n",
       "        2.03989604e-02, -1.16195149e-02,  9.21856882e-02, -6.69648121e-02,\n",
       "        7.29324436e-03,  2.93620190e-02,  6.43402617e-02,  1.59824028e-02,\n",
       "        2.18187862e-02, -1.51119323e-03, -8.90493509e-03, -1.16167957e-01,\n",
       "        3.43116624e-02, -2.22888631e-02,  6.75987527e-02,  2.60055298e-02,\n",
       "       -2.21864720e-02, -2.88950545e-02,  5.87026319e-02,  5.31624537e-05,\n",
       "        4.37127347e-02, -3.44873604e-02,  1.45444367e-02,  4.66503901e-04,\n",
       "        1.16275493e-01, -7.23772724e-02,  3.08836906e-02,  3.09007196e-03,\n",
       "        1.63774043e-02,  1.35315594e-02,  3.91931003e-02, -3.61349685e-02,\n",
       "        8.19852925e-04,  2.57454347e-04,  6.94709985e-03,  1.86907215e-02,\n",
       "       -3.81673360e-03,  1.24494154e-02, -2.23289109e-02, -3.35482652e-02,\n",
       "       -4.22900976e-02,  2.49405606e-02,  4.42143134e-03,  5.85364044e-03,\n",
       "        3.19063303e-03, -4.88638142e-02, -1.23414861e-02, -1.29953995e-02,\n",
       "        1.96110924e-02,  1.76430468e-02,  9.77955939e-03, -1.76622996e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '–ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º —Å–ª—É—à–∞—Ç–µ–ª—è–º –∫—É—Ä—Å–∞'\n",
    "get_tweet_embedding(x, model=ft, embedding_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d77a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKTEST_CCY = 'USDRUB'\n",
    "BACKTEST_DAYS = 5\n",
    "YEAR = 2022\n",
    "\n",
    "PATH_TEXTS = 'data/telegram'\n",
    "PATH_OPT_PNL = 'data/pnl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b3272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cbonds.csv',\n",
       " 'themovchans.csv',\n",
       " 'headlines_QUANTS.csv',\n",
       " 'War_Wealth_Wisdom.csv',\n",
       " 'mmi.csv',\n",
       " 'vts.csv',\n",
       " 'signal.csv',\n",
       " '.gitignore',\n",
       " 'rshb_invest.csv',\n",
       " 'Alfa_Wealth.csv',\n",
       " 'sky_bond.csv',\n",
       " 'bitkogan.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all available data_sources\n",
    "sources = os.listdir(PATH_TEXTS)\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00cad9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_start</th>\n",
       "      <th>pnl</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>4.562674e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>-4.663725e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>-4.845276e+05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>1.268498e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>-2.874466e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>-3.886725e+05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>8.040320e+05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>1.544311e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>1.868107e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2.480969e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_start           pnl  pnl_sign\n",
       "0    2022-03-02  4.562674e+06         1\n",
       "1    2022-03-03 -4.663725e+06         0\n",
       "2    2022-03-04 -4.845276e+05         0\n",
       "3    2022-03-09  1.268498e+06         1\n",
       "4    2022-03-10 -2.874466e+06         0\n",
       "..          ...           ...       ...\n",
       "138  2022-09-28 -3.886725e+05         0\n",
       "139  2022-09-29  8.040320e+05         1\n",
       "140  2022-09-30  1.544311e+06         1\n",
       "141  2022-10-03  1.868107e+06         1\n",
       "142  2022-10-04  2.480969e+06         1\n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create target variable dataframe\n",
    "pnl = pd.read_csv(f'{PATH_OPT_PNL}/Backtest_{BACKTEST_CCY}_{BACKTEST_DAYS}_days_{YEAR}.txt')\n",
    "pnl['date_start'] = pd.to_datetime(pnl['date_start']).dt.strftime('%Y-%m-%d')\n",
    "pnl['pnl_sign'] = pnl['pnl'].apply(lambda x: 1 if x >= 0 else 0)\n",
    "pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c052489b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32167832167832167"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get balance of the sample\n",
    "pnl['pnl_sign'].sum() / pnl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f953b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "def _binary_search_by_date(array: List[Tuple[dt.datetime, float]], date_x: dt.datetime) -> Union[int, None]:\n",
    "    \"\"\"\n",
    "    Searches for the index of date_x in the array via binary search.\n",
    "\n",
    "        Parameters:\n",
    "            array (list) : A sorted array of (date, float_value) tuples\n",
    "            date_x (datetime.datetime) : Date to search for\n",
    "\n",
    "        Returns:\n",
    "            index_x (int): Index of the searched date in the array.\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(array) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2\n",
    "\n",
    "        if array[mid - 1][0] <= date_x <= array[mid][0]:\n",
    "            return mid\n",
    "        elif date_x > array[mid - 1][0] and date_x > array[mid][0]:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def binary_search_time_series(time_series: List[Tuple[dt.datetime, float]], date_start: dt.datetime,\n",
    "                              date_end: dt.datetime) -> Union[List[Tuple[dt.datetime, float]], None]:\n",
    "    \"\"\"\n",
    "    Searches for the part of the time series that is contained inside [date_start; date_end] period via binary search.\n",
    "\n",
    "        Parameters:\n",
    "            time_series (list) : A sorted array of (date, float_value) tuples\n",
    "            date_start (datetime.datetime) : Starting date of the searched period\n",
    "            date_end (datetime.datetime) : Ending date of the searched period\n",
    "\n",
    "        Returns:\n",
    "            time_series_data (list): Part of the time series that is contained inside [date_start; date_end] period.\n",
    "    \"\"\"\n",
    "\n",
    "    if date_start <= date_end:\n",
    "        left_index = _binary_search_by_date(time_series, date_start)\n",
    "        \n",
    "        if left_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index = _binary_search_by_date(time_series[left_index:], date_end)\n",
    "        \n",
    "        if right_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index += left_index\n",
    "    else:\n",
    "        left_index = _binary_search_by_date(time_series, date_end)\n",
    "        \n",
    "        if left_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index = _binary_search_by_date(time_series[left_index:], date_start)\n",
    "        \n",
    "        if right_index is None:\n",
    "            return None\n",
    "        \n",
    "        right_index += left_index\n",
    "    \n",
    "    return time_series[left_index:right_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19fac87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2022, 3, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 3, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 9, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 10, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 18, 0, 0), 1),\n",
       " (datetime.datetime(2022, 3, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 24, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 25, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 30, 0, 0), 0),\n",
       " (datetime.datetime(2022, 3, 31, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 7, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 8, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 11, 0, 0), 1),\n",
       " (datetime.datetime(2022, 4, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 25, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 4, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 5, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 6, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 13, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 5, 23, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 24, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 26, 0, 0), 1),\n",
       " (datetime.datetime(2022, 5, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 10, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 24, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 6, 28, 0, 0), 1),\n",
       " (datetime.datetime(2022, 6, 29, 0, 0), 1),\n",
       " (datetime.datetime(2022, 6, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 4, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 5, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 11, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 14, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 20, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 21, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 22, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 26, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 27, 0, 0), 1),\n",
       " (datetime.datetime(2022, 7, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 7, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 1, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 3, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 4, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 10, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 11, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 12, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 15, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 16, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 17, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 18, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 19, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 22, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 23, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 24, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 25, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 29, 0, 0), 0),\n",
       " (datetime.datetime(2022, 8, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 8, 31, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 1, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 2, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 5, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 6, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 7, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 8, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 9, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 12, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 13, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 14, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 15, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 16, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 19, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 20, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 21, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 22, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 23, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 26, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 27, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 28, 0, 0), 0),\n",
       " (datetime.datetime(2022, 9, 29, 0, 0), 1),\n",
       " (datetime.datetime(2022, 9, 30, 0, 0), 1),\n",
       " (datetime.datetime(2022, 10, 3, 0, 0), 1),\n",
       " (datetime.datetime(2022, 10, 4, 0, 0), 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnl_sign_ts = [(pd.to_datetime(row['date_start']).to_pydatetime(), row['pnl_sign']) for  _, row in pnl.iterrows()]\n",
    "pnl_sign_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf278da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n",
      "/var/folders/sr/lzvmv9j54ks_jl4x99t250r00000gn/T/ipykernel_26199/371081446.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(source_data)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-07-28T10:56:14</td>\n",
       "      <td>Cbonds.ru  –∑–∞–ø—É—Å—Ç–∏–ª–æ –∫–∞–Ω–∞–ª –≤ Telegram. –ü–ª–∞–Ω–∏—Ä—É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-03T15:28:17</td>\n",
       "      <td>–û–±–ª–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –±—é–ª–ª–µ—Ç–µ–Ω—å Cbonds ‚Äì –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>2017-08-03T15:41:02</td>\n",
       "      <td>Cbonds prepared monthly report: CBONDS GLOBAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2017-08-03T18:11:32</td>\n",
       "      <td>–î–∞–π–¥–∂–µ—Å—Ç Cbonds –æ—Ç  3 –∞–≤–≥—É—Å—Ç–∞:¬´–ì—Ä—É–ø–ø–∞ –ö–æ–º–ø–∞–Ω–∏–π...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-08-04T12:42:41</td>\n",
       "      <td>–°–æ–≤–∫–æ–º–±–∞–Ω–∫ –æ—Ç–∫—Ä—ã–ª –∫–Ω–∏–≥—É –∑–∞—è–≤–æ–∫ –Ω–∞ –≤—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 date                                               text\n",
       "0   9  2017-07-28T10:56:14  Cbonds.ru  –∑–∞–ø—É—Å—Ç–∏–ª–æ –∫–∞–Ω–∞–ª –≤ Telegram. –ü–ª–∞–Ω–∏—Ä—É...\n",
       "1  10  2017-08-03T15:28:17  –û–±–ª–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –±—é–ª–ª–µ—Ç–µ–Ω—å Cbonds ‚Äì –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ ...\n",
       "2  11  2017-08-03T15:41:02  Cbonds prepared monthly report: CBONDS GLOBAL ...\n",
       "3  12  2017-08-03T18:11:32  –î–∞–π–¥–∂–µ—Å—Ç Cbonds –æ—Ç  3 –∞–≤–≥—É—Å—Ç–∞:¬´–ì—Ä—É–ø–ø–∞ –ö–æ–º–ø–∞–Ω–∏–π...\n",
       "4  13  2017-08-04T12:42:41  –°–æ–≤–∫–æ–º–±–∞–Ω–∫ –æ—Ç–∫—Ä—ã–ª –∫–Ω–∏–≥—É –∑–∞—è–≤–æ–∫ –Ω–∞ –≤—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dataframes\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in sources:\n",
    "    if s != '.gitignore':\n",
    "        source_data = pd.read_csv(f'{PATH_TEXTS}/{s}')\n",
    "        df = df.append(source_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_date = pnl_sign_ts[0][0]\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     date_x = pd.to_datetime(row['date']).to_pydatetime()\n",
    "#     pnl_sign_key = _binary_search_by_date(pnl_sign_ts, date_x)\n",
    "    \n",
    "#     if pnl_sign_key is not None and date_x >= initial_date:\n",
    "#         df.loc[df.index[i], 'pnl_sign'] = pnl_sign_ts[_binary_search_by_date(pnl_sign_ts, date_x)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d7b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_date = pnl_sign_ts[0][0]\n",
    "\n",
    "def get_pnl_sign(row):\n",
    "    date_x = pd.to_datetime(row['date']).to_pydatetime()\n",
    "    pnl_sign_key = _binary_search_by_date(pnl_sign_ts, date_x)\n",
    "    \n",
    "    if pnl_sign_key is not None and date_x >= initial_date:\n",
    "        return pnl_sign_ts[_binary_search_by_date(pnl_sign_ts, date_x)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75961477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2017-07-28T10:56:14</td>\n",
       "      <td>Cbonds.ru  –∑–∞–ø—É—Å—Ç–∏–ª–æ –∫–∞–Ω–∞–ª –≤ Telegram. –ü–ª–∞–Ω–∏—Ä—É...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2017-08-03T15:28:17</td>\n",
       "      <td>–û–±–ª–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –±—é–ª–ª–µ—Ç–µ–Ω—å Cbonds ‚Äì –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>2017-08-03T15:41:02</td>\n",
       "      <td>Cbonds prepared monthly report: CBONDS GLOBAL ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2017-08-03T18:11:32</td>\n",
       "      <td>–î–∞–π–¥–∂–µ—Å—Ç Cbonds –æ—Ç  3 –∞–≤–≥—É—Å—Ç–∞:¬´–ì—Ä—É–ø–ø–∞ –ö–æ–º–ø–∞–Ω–∏–π...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>2017-08-04T12:42:41</td>\n",
       "      <td>–°–æ–≤–∫–æ–º–±–∞–Ω–∫ –æ—Ç–∫—Ä—ã–ª –∫–Ω–∏–≥—É –∑–∞—è–≤–æ–∫ –Ω–∞ –≤—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 date                                               text  \\\n",
       "0   9  2017-07-28T10:56:14  Cbonds.ru  –∑–∞–ø—É—Å—Ç–∏–ª–æ –∫–∞–Ω–∞–ª –≤ Telegram. –ü–ª–∞–Ω–∏—Ä—É...   \n",
       "1  10  2017-08-03T15:28:17  –û–±–ª–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –±—é–ª–ª–µ—Ç–µ–Ω—å Cbonds ‚Äì –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ ...   \n",
       "2  11  2017-08-03T15:41:02  Cbonds prepared monthly report: CBONDS GLOBAL ...   \n",
       "3  12  2017-08-03T18:11:32  –î–∞–π–¥–∂–µ—Å—Ç Cbonds –æ—Ç  3 –∞–≤–≥—É—Å—Ç–∞:¬´–ì—Ä—É–ø–ø–∞ –ö–æ–º–ø–∞–Ω–∏–π...   \n",
       "4  13  2017-08-04T12:42:41  –°–æ–≤–∫–æ–º–±–∞–Ω–∫ –æ—Ç–∫—Ä—ã–ª –∫–Ω–∏–≥—É –∑–∞—è–≤–æ–∫ –Ω–∞ –≤—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞...   \n",
       "\n",
       "   pnl_sign  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pnl_sign'] = df.apply(lambda row: get_pnl_sign(row), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02b786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>pnl_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>2022-03-02T09:38:25</td>\n",
       "      <td>–£–¢–†–ï–ù–ù–ò–ô –î–ê–ô–î–ñ–ï–°–¢  üèõ‚ùóÔ∏è–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>2022-03-02T10:49:57</td>\n",
       "      <td>#–ù–æ–≤–æ—Å—Ç–∏–ö–æ–º–ø–∞–Ω–∏–π  ‚ö°Ô∏è –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: –æ—Å–Ω–æ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10815</td>\n",
       "      <td>2022-03-02T11:06:47</td>\n",
       "      <td>‚ö°Ô∏è –í–∞–∂–Ω–æ–µ –Ω–∞ —Ä—ã–Ω–∫–∞—Ö: ‚õîÔ∏è–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10816</td>\n",
       "      <td>2022-03-02T11:23:57</td>\n",
       "      <td>üìù –ù–ê–£–§–û–† –ø—Ä–æ—Å–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10817</td>\n",
       "      <td>2022-03-02T12:23:55</td>\n",
       "      <td>#–î–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫ üìÜ üí∞–°–æ–±—ã—Ç–∏—è –¥–µ–Ω–µ–∂–Ω–æ–≥–æ —Ä—ã–Ω–∫–∞ —Å–µ–≥–æ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                 date  \\\n",
       "0  10813  2022-03-02T09:38:25   \n",
       "1  10814  2022-03-02T10:49:57   \n",
       "2  10815  2022-03-02T11:06:47   \n",
       "3  10816  2022-03-02T11:23:57   \n",
       "4  10817  2022-03-02T12:23:55   \n",
       "\n",
       "                                                text  pnl_sign  \n",
       "0  –£–¢–†–ï–ù–ù–ò–ô –î–ê–ô–î–ñ–ï–°–¢  üèõ‚ùóÔ∏è–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω...       0.0  \n",
       "1  #–ù–æ–≤–æ—Å—Ç–∏–ö–æ–º–ø–∞–Ω–∏–π  ‚ö°Ô∏è –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: –æ—Å–Ω–æ...       0.0  \n",
       "2  ‚ö°Ô∏è –í–∞–∂–Ω–æ–µ –Ω–∞ —Ä—ã–Ω–∫–∞—Ö: ‚õîÔ∏è–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏...       0.0  \n",
       "3  üìù –ù–ê–£–§–û–† –ø—Ä–æ—Å–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å...       0.0  \n",
       "4  #–î–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫ üìÜ üí∞–°–æ–±—ã—Ç–∏—è –¥–µ–Ω–µ–∂–Ω–æ–≥–æ —Ä—ã–Ω–∫–∞ —Å–µ–≥–æ...       0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['pnl_sign'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e7f8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id', 'date', 'pnl_sign'], axis=1)\n",
    "y = df['pnl_sign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f3545b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        –£–¢–†–ï–ù–ù–ò–ô –î–ê–ô–î–ñ–ï–°–¢  üèõ‚ùóÔ∏è–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω...\n",
       "1        #–ù–æ–≤–æ—Å—Ç–∏–ö–æ–º–ø–∞–Ω–∏–π  ‚ö°Ô∏è –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: –æ—Å–Ω–æ...\n",
       "2        ‚ö°Ô∏è –í–∞–∂–Ω–æ–µ –Ω–∞ —Ä—ã–Ω–∫–∞—Ö: ‚õîÔ∏è–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏...\n",
       "3        üìù –ù–ê–£–§–û–† –ø—Ä–æ—Å–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å...\n",
       "4        #–î–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫ üìÜ üí∞–°–æ–±—ã—Ç–∏—è –¥–µ–Ω–µ–∂–Ω–æ–≥–æ —Ä—ã–Ω–∫–∞ —Å–µ–≥–æ...\n",
       "                               ...                        \n",
       "14922    –ó–∞–≤–µ—Ä—à–∞–µ–º –Ω–∞—á–∞—Ç—É—é –≤ —Å—É–±–±–æ—Ç—É —Å–µ—Ä–∏—é  –∑–∞–º–µ—Ç–æ–∫  –æ ...\n",
       "14923    –°–∞–Ω–∫—Ü–∏–∏ –Ω–∞ –ù–ö–¶ = —Å–∞–Ω–∫—Ü–∏–∏ –Ω–∞ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ –∞–∫—Ü–∏–∏...\n",
       "14924    –í —Å—Ä–µ–¥—É –≤ –í–µ–Ω–µ –≥—Ä—É–ø–ø–∞ –û–ü–ï–ö+ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—Å—è, —á—Ç–æ–±—ã ...\n",
       "14925    –ù–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ –≤  —Å–µ—Ä–≤–∏—Å–µ –ø–æ –ø–æ–¥–ø–∏—Å–∫–µ  –º—ã –∑...\n",
       "14926    –¢–æ—Ä–≥–∏ –Ω–∞ –≤–∞–ª—é—Ç–Ω–æ–º —Ä—ã–Ω–∫–µ —Å–µ–≥–æ–¥–Ω—è –ø—Ä–æ—Ö–æ–¥—è—Ç –Ω–µ—Ç–∏–ø...\n",
       "Name: text, Length: 14927, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6e3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from functools import lru_cache\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.gui import tqdm as tqdm_gui\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "regex = re.compile(\"[–∞-—èa-z—ë–Å]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]\n",
    "\n",
    "mystopwords = stopwords.words('russian') \n",
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return ' '.join(remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c88026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b2f9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd482ae030cb46819b6f3991395a1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>#–ú–∞–∫—Ä–æ \\n üìä –û–ø—Ä–æ—Å: –ú–∞—Å–∫ ‚Äî –∫—Ä–∞—Å–∞–≤—á–∏–∫? –í –∞–ø—Ä–µ–ª–µ ...</td>\n",
       "      <td>–º–∞–∫—Ä–æ –æ–ø—Ä–æ—Å –º–∞—Å–∫ –∫—Ä–∞—Å–∞–≤—á–∏–∫ –∞–ø—Ä–µ–ª—å –æ–ø—Ä–æ—Å–∏—Ç—å –æ—Ç–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10636</th>\n",
       "      <td>#–ú–∞–∫—Ä–æ \\nüá∑üá∫ –°–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –†–æ—Å—Å–∏–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ...</td>\n",
       "      <td>–º–∞–∫—Ä–æ —Å–∞–Ω–∫—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ —Ä–æ—Å—Å–∏—è —Å–º–æ—á—å –ø–æ–¥–æ—Ä–≤–∞—Ç—å —Ñ–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>#Commodities \\nüìä  –¢–æ–≤–∞—Ä–Ω—ã–µ   –±–∏—Ä–∂–∏ –¶–µ–Ω—ã —Ñ—å—é—á–µ—Ä...</td>\n",
       "      <td>commodities —Ç–æ–≤–∞—Ä–Ω—ã–π –±–∏—Ä–∂–∞ —Ü–µ–Ω–∞ —Ñ—å—é—á–µ—Ä—Å –æ—Å–Ω–æ–≤–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>#Commodities \\nüìä  –¢–æ–≤–∞—Ä–Ω—ã–µ   –±–∏—Ä–∂–∏ –¶–µ–Ω—ã —Ñ—å—é—á–µ—Ä...</td>\n",
       "      <td>commodities —Ç–æ–≤–∞—Ä–Ω—ã–π –±–∏—Ä–∂–∞ —Ü–µ–Ω–∞ —Ñ—å—é—á–µ—Ä—Å –æ—Å–Ω–æ–≤–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>‚ÄºÔ∏è –í—Å–µ –∫–∞—Ä—Ç—ã –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º Vi...</td>\n",
       "      <td>–∫–∞—Ä—Ç–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –ø–ª–∞—Ç—ë–∂–Ω—ã–π —Å–∏—Å—Ç–µ–º–∞ visa mas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "9288   #–ú–∞–∫—Ä–æ \\n üìä –û–ø—Ä–æ—Å: –ú–∞—Å–∫ ‚Äî –∫—Ä–∞—Å–∞–≤—á–∏–∫? –í –∞–ø—Ä–µ–ª–µ ...   \n",
       "10636  #–ú–∞–∫—Ä–æ \\nüá∑üá∫ –°–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –†–æ—Å—Å–∏–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ...   \n",
       "4785   #Commodities \\nüìä  –¢–æ–≤–∞—Ä–Ω—ã–µ   –±–∏—Ä–∂–∏ –¶–µ–Ω—ã —Ñ—å—é—á–µ—Ä...   \n",
       "8707   #Commodities \\nüìä  –¢–æ–≤–∞—Ä–Ω—ã–µ   –±–∏—Ä–∂–∏ –¶–µ–Ω—ã —Ñ—å—é—á–µ—Ä...   \n",
       "2392   ‚ÄºÔ∏è –í—Å–µ –∫–∞—Ä—Ç—ã –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º Vi...   \n",
       "\n",
       "                                                  lemmas  \n",
       "9288   –º–∞–∫—Ä–æ –æ–ø—Ä–æ—Å –º–∞—Å–∫ –∫—Ä–∞—Å–∞–≤—á–∏–∫ –∞–ø—Ä–µ–ª—å –æ–ø—Ä–æ—Å–∏—Ç—å –æ—Ç–∑...  \n",
       "10636  –º–∞–∫—Ä–æ —Å–∞–Ω–∫—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ —Ä–æ—Å—Å–∏—è —Å–º–æ—á—å –ø–æ–¥–æ—Ä–≤–∞—Ç—å —Ñ–∏...  \n",
       "4785   commodities —Ç–æ–≤–∞—Ä–Ω—ã–π –±–∏—Ä–∂–∞ —Ü–µ–Ω–∞ —Ñ—å—é—á–µ—Ä—Å –æ—Å–Ω–æ–≤–Ω...  \n",
       "8707   commodities —Ç–æ–≤–∞—Ä–Ω—ã–π –±–∏—Ä–∂–∞ —Ü–µ–Ω–∞ —Ñ—å—é—á–µ—Ä—Å –æ—Å–Ω–æ–≤–Ω...  \n",
       "2392   –∫–∞—Ä—Ç–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –ø–ª–∞—Ç—ë–∂–Ω—ã–π —Å–∏—Å—Ç–µ–º–∞ visa mas...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = list(tqdm(map(clean_text, X_train['text']), total=len(X_train)))\n",
    "\n",
    "X_train['lemmas'] = lemmas\n",
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7159b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15375699ad784bee92dda85176e31cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmas_val = list(tqdm(map(clean_text, X_val['text']), total=len(X_val)))\n",
    "\n",
    "X_val['lemmas'] = lemmas_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84cde13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11941, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e76422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cc1815d7a84f959367fe1fc3242f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "X_train['embedding'] = X_train['lemmas'].progress_apply(lambda x: get_tweet_embedding(x, model=ft))\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4b9cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c82e46dff9b45fb89bc64c9dc4acfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "X_val['embedding'] = X_val['lemmas'].progress_apply(lambda x: get_tweet_embedding(x, model=ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c58229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding = np.array(list(X_train['embedding'].values))\n",
    "val_embedding = np.array(list(X_val['embedding'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ca0cbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993551628841805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(random_state=12)\n",
    "clf.fit(train_embedding, y_train)\n",
    "clf.score(train_embedding, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfae93e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999330207635633"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(val_embedding)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1ae4906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695244474212994"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features=4, n_estimators=50)\n",
    "\n",
    "rf.fit(train_embedding, y_train)\n",
    "(y_val == rf.predict(val_embedding)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f27bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32cdbf9fca41cb959bb02b9b0dc41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_embedding, label=y_val)\n",
    "model.fit(train_embedding, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_embedding)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = list(tqdm(map(clean_text, df['text']), total=len(df)))\n",
    "\n",
    "df['lemmas'] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344297db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a423224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_ft.txt', 'w') as f:\n",
    "    for label, lemmas in list(zip(\n",
    "        y_train, X_train['lemmas']\n",
    "    )):\n",
    "        f.write(f\"__label__{int(label)} {lemmas}\\n\")\n",
    "        #print(f\"__label__{int(label)} {lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25163fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail train_ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.train_supervised('train_ft.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(list(X_val['lemmas']))[0]\n",
    "pred = [int(label[0][-1]) for label in pred]\n",
    "\n",
    "accuracy_score(list(y_val), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [tweet.split() for tweet in train['lemmas'].values]\n",
    "\n",
    "%time w2v = word2vec.Word2Vec(tokenized_tweets, workers=4, vector_size=200, min_count=10, window=3, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['—Ä–æ—Å—Ç'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(lemmas, model=w2v.wv, embedding_size=200):\n",
    "    \n",
    "    res = np.zeros(embedding_size)\n",
    "    cnt = 0\n",
    "    for word in lemmas.split():\n",
    "        if word in model:\n",
    "            res += np.array(model[word])\n",
    "            cnt += 1\n",
    "    if cnt:\n",
    "        res = res / cnt\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c181c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['w2v_embedding'] = X_train['lemmas'].map(get_tweet_embedding)\n",
    "X_val['w2v_embedding'] = X_val['lemmas'].map(get_tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4694775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = list(X_train['w2v_embedding'].values)\n",
    "val_w2v = list(X_val['w2v_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=12, max_iter=500)\n",
    "clf.fit(train_w2v, y_train)\n",
    "\n",
    "pred = clf.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_w2v, label=y_val)\n",
    "model.fit(train_w2v, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm  \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# import dlnlputils\n",
    "# from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "#     vectorize_texts, SparseFeaturesDataset\n",
    "# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "# init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86569712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token=''):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # –ø–æ—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É–ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç—Å—è –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ\n",
    "    # –∞ —Ç–∞–∫–∂–µ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # —É–±—Ä–∞—Ç—å —Å–ª–∏—à–∫–æ–º —Ä–µ–¥–∫–∏–µ –∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # –¥–æ–±–∞–≤–∏–º –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–ª–æ–≤–æ —Å –∏–Ω–¥–µ–∫—Å–æ–º 0 –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # –µ—Å–ª–∏ —É –Ω–∞—Å –ø–æ –ø—Ä–µ–∂–Ω–µ–º—É —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–ª–æ–≤, –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ max_size —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # –Ω—É–º–µ—Ä—É–µ–º —Å–ª–æ–≤–∞\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # –Ω–æ—Ä–º–∏—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # –ø–æ–ª—É—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ \"–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∏–ª–∏ –Ω–µ—Ç\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π —Å–ª–æ–≤–∞ –≤ –¥–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ,\n",
    "    # –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ –≤ —Ü–µ–ª–æ–º\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # —É—á–∏—Ç—ã–≤–∞–µ–º –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —É –Ω–∞—Å –µ—Å—Ç—å:\n",
    "    # —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏ —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))  # —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ –µ—ë –¥–ª–∏–Ω—É\n",
    "        result = result.multiply(1 / word2freq)  # —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–∞–∂–¥—ã–π —Å—Ç–æ–ª–±–µ—Ü –Ω–∞ –≤–µ—Å —Å–ª–æ–≤–∞\n",
    "\n",
    "    if scale:\n",
    "        result = result.tocsc()\n",
    "        result -= result.min()\n",
    "        result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result.tocsr()\n",
    "\n",
    "\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenize_corpus(X_train['text'])\n",
    "val_tokenized = tokenize_corpus(X_val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(train_tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a12687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å - vocabulary —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ build_vocabulary\n",
    "# –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Ç–æ–∫–µ–Ω–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ\n",
    "# word_doc_freq - —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –≤—Å–µ—Ö —Å–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –æ–Ω –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è \n",
    "# –Ω–∞ —ç—Ç–∞–ø–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "MAX_DF = 0.8 #–≤–æ —Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—å—Å—è —Å–ª–æ–≤–æ\n",
    "MIN_COUNT = 5 # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—å—Å—è –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "\n",
    "\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
    "UNIQUE_WORDS_N = len(vocabulary)\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤', UNIQUE_WORDS_N)\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(word_doc_freq, bins=20)\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZATION_MODE = 'tfidf'\n",
    "# –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –º–µ—Ç–æ–¥—É –º–µ—à–∫–∞ —Å–ª–æ–≤\n",
    "# —Ñ—É–Ω–∫—Ü–∏—è vectorize_texts –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥\n",
    "#1. —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤\n",
    "#2. —Å–ª–æ–≤–∞—Ä—å\n",
    "#3. –≤–µ–∫—Ç–æ—Ä —á–∞—Å—Ç–æ—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã\n",
    "#4. –∞–ª–≥–æ—Ä–∏—Ç–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ mode - –µ—Å—Ç—å 4 –∞–ª–≥–æ—Ä–∏–º–∞ - bin,tf,idf,tfidf\n",
    "#5. —Ñ–ª–∞–≥ —á—Ç–æ–±—ã –ø–µ—Ä–µ–º–∞—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Ñ–ª–∞–≥ –ø–æ—Å–ª–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è\n",
    "\n",
    "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
    "\n",
    "print('–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏', train_vectors.shape)\n",
    "print()\n",
    "print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ', train_vectors.nnz)\n",
    "print('–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa419ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def get_vectors_gt100(row):\n",
    "    '''\n",
    "      word_doc_freq # —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
    "      train_tokenized #—Å–∞–º–∏ —Å–ª–æ–≤–∞\n",
    "    '''\n",
    "    vecs = [np.zeros(100)]\n",
    "    for word in row:\n",
    "        #print(row)\n",
    "        try: \n",
    "            # –µ—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ –Ω–∞—à–µ–º –æ—á–∏—â–µ–Ω–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ\n",
    "            # —É–º–Ω–æ–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä –Ω–∞ –≤–µ—Å tfidf\n",
    "            v = model_t[word] * word_doc_freq[vocabulary[word]] \n",
    "        except:\n",
    "            v = np.zeros(100)\n",
    "        vecs.append(v)\n",
    "    return np.sum(np.array(vecs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt100 = np.array([get_vectors_gt100(i) for i in train_tokenized])\n",
    "val_gt100 = np.array([get_vectors_gt100(i) for i in val_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.most_similar(positive=['–∏–Ω–≤–µ—Å—Ç–æ—Ä', '—Ä—ã–Ω–æ–∫'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd22560",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=12, max_iter=500)\n",
    "clf.fit(train_gt100, y_train)\n",
    "\n",
    "pred = clf.predict(val_gt100)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c095ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "params = dict(\n",
    "    learning_rate=0.025,\n",
    "    iterations=10000,\n",
    "    reg_lambda=0.0005,\n",
    "    colsample_bylevel=1.,\n",
    "    max_bin=80,\n",
    "    bagging_temperature=2,\n",
    "    use_best_model=True,\n",
    "    verbose=False,\n",
    "    grow_policy='Depthwise',\n",
    "    random_seed=12\n",
    ")\n",
    "model = cb.CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "eval_set = cb.Pool(data=val_gt100, label=y_val)\n",
    "model.fit(train_gt100, y_train, eval_set=eval_set, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81132739",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_w2v)\n",
    "accuracy_score(pred, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
